# Introducing FlowFrame

FlowFrame enforces information flow control policies in Scala Spark
applications, specified by Purpose Policies.  
FlowFrame enforces policies statically at compile time.
This means programmers get feedback earlier and helps them write
privacy-enforcing code from the start.  Another big advantage is that
**enforcing policies at compile time means *little-to-no runtime overhead* for
enforcement.** Since the compiler itself is enforcing policies for all possible
runs of the application, the computation effort of checking policies is
performed on developer machines rather than performance-critical service nodes,
eliminating many scalability issues.  Even more importantly, ***FlowFrame
enforces privacy policies on all Scala Spark application code, even
user-defined functions written in Scala.***

## FlowFrame Basics

Currently, to take advantage of FlowFrame, the Spark application must (in
general) restrict itself to the typed fragment of the [Dataset
API](https://spark.apache.org/docs/2.2.0/sql-programming-guide.html#datasets-and-dataframes).
This API allows data loaded from tables to be associated with user-defined
structures in Scala.  For example, suppose an application reads from a table
`foo_bar` with columns `foo:Long` , `bar:Double`, and `ds:String`.  Using the
Dataset API, rows from this table can be processed as a collection of
`FooBarTable` values, as defined below.  The types defined by the `FooBarTable`
case class prevent mismatches (by throwing an exception) between the data
format expected by the program and the actual data format stored in the
database.

FlowFrame takes this a step further with *policy annotations* which not only
ensure the expected policies on tables and columns correspond to their actual
labels, but only ensure that the flow of information within the Spark
application respects these policies—both in the SQL generated by the Dataset
API, as well as the native Scala code in user-defined functions.

```
@policy("any") // table policy
case class FooBarTable(
   foo: Long    @policy("fb::alice"),  // column policy
   bar: Double  @policy("fb::bob"),
   ds: String   // default: @policy("any")
)
```

The annotation on the case class represents a table-level
policy, annotations on field types correspond to column-level policies.
Unannotated classes and fields receive the most permissive policy, `any`, by
default.  When accessing a field, e.g., `df.foo`, the policy on the expression
`df.foo` is the upperbound of the class and field policies, in this case `any
and fb::alice`. Eventually, we hope for these annotations, as well as the case
class definitions themselves, to be produced automatically using 
tools that generate code from the metadata and schemas of the tables themselves.

Output tables are defined similarly.  Suppose the FooBar application filters
and processes some of the rows from `foo_bar` and outputs the results to table
`bar`.  The output rows can be represented as a collection of `BarTable`
values.  The goal of FlowFrame is to ensure that the application only attempts
to write rows to `bar` that depend on data from `foo_bar` (or other input
tables) labeled with policies that *flows to* `fb::bob`.

```
@policy("any")
case class BarTable(
   bar: Double  @policy("fb::bob")
)
```

Informally, a policy *p* flows to a policy *q* if *q* is *at least as
restrictive* as *p.* This
means that treating data associated with policy p as though it had policy q is
secure, although potentially more conservative.  For example, data allowed for
purpose `fb::alice or fb::bob`  can safely flow to a table with policy
`fb::bob` since that purpose was allowed under the original policy.  However,
the data in the new table will not be allowed for a `fb::alice` purpose since
other data in the table may not have permitted that purpose.  Additional
purpose categories ***increase*** restrictiveness: `fb` flows to `fb::bob`, but
not vice-versa.

Typically, data is loaded as a collection of tuples (i.e., `Dataset[Row]`) then
associated with the desired type using the `as` API call.

```
object FooBarTable {
  def apply(spark: SparkSession): Dataset[FooBarTable] = {
    import spark.implicits._
    spark.table("foo_bar").as[FooBarTable]
  }
}
```

FlowFrame adds a new pass to the Scala compiler that associates policies with a
program’s definitions and expressions.  The `as` API call is the initial source
of these policies, and the annotated policies on the type argument to `as`
should correspond to the policies defined on the underlying table’s metadata.
Our current design uses [`case
class`](https://docs.scala-lang.org/overviews/scala-book/case-classes.html)
definitions,  policy annotations, and table loading code to track input tables
and output tables.  Policies (and schema) may change over time, however, so
additional mechanisms may be necessary to prevent policies from diverging after
a FlowFrame application has been verified, compiled, and deployed.  On the
FlowFrame side, compile-time or commit-time checks can (but do not currently)
ensure that the code’s policies have not diverged .  Similar checks should also
be performed whenever a table’s policies are updated.

### Your first (of many) compile-time privacy exception

To understand how FlowFrame works, let’s examine a simple program that filters
and transforms data in `foo_bar` and saves the output to table `bar`. 

```
val table = FooBarTable(spark)
val df = table.filter(_.foo > 1337).map((r:FooBarTable) => BarTable(r.bar + 42))
df.write.saveAsTable("bar")
```

First, the table is loaded by passing the current Spark session into
`FooBarTable`’s `apply` method.  Then, using `filter` and `map`, the program
selects a subset of rows based on the `foo` column, and outputs the row’s `bar`
value plus 42. The resulting Dataset is then saved to table `bar`.

Is this program secure?  Let’s ask FlowFrame!  Building this code with the
FlowFrame plugin enabled the (something like) following compiler error:

```
Unsatisfiable constraint: (({select:FooBarTest.type <= fb::alice}) and (fb::bob)) and ({r <= fb::alice}) <= fb::bob
```

for the line declaring and initializing `df`.  So why does FlowFrame think this
code is problematic?  Looking at the `map` function, we see the `BarTable`
elements are constructed from the `bar` field of each passed-in `FooBarTable`
row. Therefore, we should make sure the policy on `FooBarTable.bar` flows to
`BarTable.bar`. Reviewing their definitions above, we see that both are labeled
`fb:bob`. Therefore, it doesn’t seem like an illegal *explicit* flow is the
problem.

Now consider the `filter` call.  Before `map` is called, the elements of
`FooBarTable` are filtered based on the `foo` field, so the rows that are
passed to the `map` function are those that have a `foo` value greater than
`1337`.  But the policy on `FooBarTable.foo` is `fb::alice`!  This means that
*membership* in the filtered collection of rows should be protected at policy
`fb::alice`.  Since `42` is only added to the subset of rows matching the
filter predicate, an *implicit* flow from `FooBarTable.foo` to `BarTable.bar`
is present that violates `fb::bob`, the policy on `BarTable.bar`.

### Deconstructing the unsatisfiable constraints

Let’s look more closely at the (WIP) error message FlowFrame gave us for the
above example.  The rather verbose labels in the curly brackets (e.g., `{r <=
fb::alice}`) represent *abstract policies*: or policies whose *concrete value*
is not known at compile time.  Abstract policies allow code reuse in the same
way that abstract type parameters or generics do: we can write code once and
use it in multiple contexts.  

Even though the concrete policies are unknown, we can still reason about the
relationship between abstract policies and other (abstract *or* concrete)
policies.  Consider `{r <= fb::alice}`. The `<=` denotes that the right-hand
side is an *upper bound* for the abstract policy `r`, which represents the
policy on each row the lambda function passed to `map` is applied to.  This is
how the illegal flow is captured: even though the table-level policy on
`FooBarTable` is public, since the subset of rows passed to `map` is filtered
on field `foo`, which has policy `fb::alice`, the *reference* to each row in
that subset has policy `fb::alice` too.  Therefore, the expression `r.bar + 42`
is at least as restrictive as `fb::alice and fb::bob` because of the policies
on `r` and `r.bar`.

### Inferring policies with constraint solving

For FlowFrame to catch policy violations, all variables and expressions must
have an associated privacy policy.  Adding policy annotations by hand would be
incredibly time-consuming, so FlowFrame attempts to infer policies
automatically whenever possible.  ***A design goal for FlowFrame is to infer
most policies in Spark applications***, as long as the input and output tables
are annotated.  FlowFrame infers policies using a custom constraint solver
based heavily on the solver used in the [JIF
compiler](https://www.cs.cornell.edu/jif/).  

For example, consider the code below.  The variable `x` is not annotated with a
policy, so FlowFrame tries to infer it.  Since `x` ‘s value depends on `r.bar`,
there is an implicit flow from `r.bar` to `x` created by the assignments `x =
0` and `x = 1` .  FlowFrame represents the policy on `x` as a variable to be
solved for, and represents the assignments as constraints on valid solutions.
Specifically, if we write the policy on `x` as `{x}`, then if a valid solution
for `{x}` exists, it must be the case that `fb::alice and fb::bob`, the policy
on `r.bar`, must flow to `{x}`, or `fb::alice and fb::bob <= {x}`. 

```
var x = 0 // unlabeled variables and parameters are inferred  
if (r.bar > 42)
  x = 0
else  
  x = 1
```

Similar to a type inference algorithm, the compiler collects the constraints
for a program, attempts to find a solution for all the unknown variables, and
then applies that solution to get a fully-annotated, secure program.  If it
cannot find a solution that satisfies all constraints, it rejects the program
as insecure.

## Status 

FlowFrame’s constraint solver and policy parser are complete,  and a
significant subset of Scala features are supported by the policy checker, but
only a subset of the Spark Dataset API is currently supported.  It should be
possible to expand support now that the basic infrastructure is in place,
but that is a work in progress.
